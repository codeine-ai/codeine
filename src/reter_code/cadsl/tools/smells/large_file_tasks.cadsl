# large_file_tasks - Detect files too large for AI tools and create review tasks
#
# AI coding assistants have limited context windows. Large files require
# chunked reading, lose context between chunks, and make refactoring error-prone.
#
# Creates tasks for human review and classification (TP/FP).
#
# Uses file_scan for accurate line counts (not MAX(?end_line) approximation).

detector large_file_tasks(category="smell-review", severity="high") {
    """
    Find files exceeding size/complexity thresholds for AI tool optimization.

    Each task requires manual classification:
    - TP: File should be split into smaller, focused modules
    - FP-GENERATED: Auto-generated code (ANTLR, protobuf)
    - FP-COHESIVE: Type hierarchy that would fragment if split (ALL: shared base, polymorphic use, always together)
    - FP-TEST-FILE: Test file with many test classes
    - FP-SINGLE-MODULE: Language convention (package __init__.py)
    - FP-DATA-DEFINITIONS: Pure data definitions (enums, constants)

    NOTE: FP-COHESIVE requires ALL criteria: shared base class, type hierarchy, always used together.
    "Related classes" or "same domain" is NOT enough - classify as TP instead.

    Parameters:
        max_lines: Maximum lines before flagging (default: 500)
        max_classes: Maximum classes per file before flagging (default: 1)
        max_functions: Maximum top-level functions before flagging (default: 10)
        limit: Maximum tasks to create (default: 50)
        dry_run: If true, preview without creating tasks (default: true)
    """

    param max_lines: int = 500;
    param max_classes: int = 1;
    param max_functions: int = 10;
    param limit: int = 50;
    param dry_run: bool = true;

    # Get actual file stats from RETER sources
    # file_scan iterates over loaded sources, provides real line counts
    file_scan {
        glob: "*"
    }
    | filter {
        (file contains ".py" or file contains ".ts" or file contains ".js" or
         file contains ".cpp" or file contains ".h" or file contains ".cs" or file contains ".hpp") and
        not (file contains "/test/" or file contains "/tests/" or
             file contains "_test.py" or file contains "test_" or
             file contains "__pycache__" or file contains "node_modules" or
             file contains ".generated.")
    }
    # Deduplicate files (sources may have multiple entries per file)
    | unique { file }
    | join {
        on: file,
        right: reql {
            SELECT ?file (COUNT(DISTINCT ?c) AS ?class_count)
            WHERE {
                ?c type class .
                ?c is-in-file ?file
            }
            GROUP BY ?file
        },
        type: left
    }
    | join {
        on: file,
        right: reql {
            SELECT ?file (COUNT(?f) AS ?function_count)
            WHERE {
                ?f type function .
                ?f is-in-file ?file
            }
            GROUP BY ?file
        },
        type: left
    }
    | map {
        file: file,
        line_count: line_count,
        file_size: file_size,
        class_count: class_count ?? 0,
        function_count: function_count ?? 0
    }
    | filter { class_count > {max_classes} || function_count > {max_functions} || line_count > {max_lines} }
    | order_by { -line_count }
    | limit { {limit} }
    | create_task {
        name: "[large_file] {file} ({line_count} lines, {class_count} classes)",
        category: "smell-review",
        priority: high,
        description: "**Large File detected** - requires CODE ANALYSIS before classification.\n\n**File**: `{file}`\n**Lines**: {line_count}\n**Size**: {file_size} bytes\n**Classes**: {class_count}\n**Top-level Functions**: {function_count}\n\n---\n\n## ⚠️ CRITICAL: Read the Code First!\n\n**You MUST read the actual file content before classifying!**\n\n```python\n# ALWAYS read the file first - never classify based on filename alone!\nRead(file_path=\"{file}\")\n\n# For large files, read in chunks:\nRead(file_path=\"{file}\", limit=300)  # First 300 lines\nRead(file_path=\"{file}\", offset=300, limit=300)  # Next chunk\n```\n\n**What to look for:**\n- What do the classes actually do? (Read the code!)\n- Are they tightly coupled or independent?\n- Is there a shared base class?\n- Are there \"generated by\" comments?\n- What imports/includes are used?\n\n---\n\n## Why This Matters for AI Tools\n\nAI coding assistants have limited context windows:\n- Large files require chunked reading\n- Context is lost between chunks\n- Refactoring becomes error-prone\n\n**Target**: Each file should be <500 lines with single responsibility.\n\n---\n\n## Classification (After Reading Code!)\n\n**Step 1: Rule out False Positives** (based on actual code analysis)\n\n| Check | Question | How to Verify |\n|-------|----------|---------------|\n| FP-GENERATED | Auto-generated code? | Look for generator comments, repetitive patterns |\n| FP-COHESIVE | Type hierarchy? (STRICT) | ALL: shared base, polymorphic, always together |\n| FP-TEST-FILE | Test file? | Look for test imports, test methods |\n| FP-SINGLE-MODULE | Package export file? | Check if __init__.py or entry point |\n| FP-DATA-DEFINITIONS | Pure data classes? | Check if classes have only data, no logic |\n\n**Step 2: If no FP criteria met → TRUE POSITIVE**\n\n---\n\n## If TP - Split Planning\n\n1. **List each class** and its responsibility (from code analysis)\n2. **Group functions** by purpose\n3. **Find boundaries**: Where are natural split points?\n4. **Plan new files**: Name by content\n5. **Check dependencies**: Note circular deps to fix first\n\n## Split Candidates\n\n- Each class → separate file\n- Utility functions → `*_helpers.py` or `*_utils.py`\n- Data classes/DTOs → `*_types.py` or `*_models.py`\n- Constants/enums → `*_constants.py`",
        affects: file,
        dry_run: {dry_run}
    }
    | emit { tasks }
}
